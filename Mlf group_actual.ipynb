{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1>Project Proposal:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h2>Objective</h2>\n",
    "\n",
    "#### <h3>Problem statement</h3>\n",
    "\n",
    "<p>Among the many debt instruments financial institutions use, personal loans are a significant part of their retail banking portfolio. These loans contribute to revenue through higher interest rates and serve as a key tool for meeting individual consumer credit needs. Defaulting on these loans can pose a significant risk to financial institutions as they can lead to revenue loss, increased operational costs and strained relationships with borrowers. Therefore, identifying high-risk borrowers early on is critical to mitigating risk and improving the loan management process. With the help of machine learning, we aim to build a predictive model that assesses the probability of a loan defaulting in order to assist financial institutions make better data-driven decisions. \n",
    "</p>\n",
    "\n",
    "#### <h3>Goals</h3>\n",
    "\n",
    "<ul>\n",
    "<li> Build a machine learning model to predict whether a loan will default.</li>\n",
    "<li>Identify the most significant features influencing loan defaulting.</li>\n",
    "<li>Provide actionable insights to enhance the risk management process for financial institutions.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h2>Deliverables:</h2>\n",
    "\n",
    "#### <h3>Scope</h3>\n",
    "\n",
    "<ul>\n",
    "<li>Data preparation and cleaning. </li>\n",
    "<li>Exploratory Data Analysis (EDA) to visualize trends and patterns.</li>\n",
    "<li>Building and evaluating multiple machine learning models for classification.</li>\n",
    "<li>Deployment of a final model as a REST API for loan risk predictions.</li>\n",
    "</ul>\n",
    "\n",
    "#### <h3>Dataset</h3>\n",
    "\n",
    "<ul>\n",
    "<li>Source: Lending Club Loan Data from Kaggle.</li>\n",
    "<li>Description: Contains details on loan applicants, loan amounts, terms, interest rates, payment histories, and outcomes.</li>\n",
    "</ul>\n",
    "\n",
    "#### <h3>Deliverables</h3>\n",
    "<ul> <li>A cleaned and preprocessed dataset ready for analysis.</li>\n",
    "<li>Visualizations and statistical insights from EDA.</li>\n",
    "<li>A predictive model with evaluation results.</li>\n",
    "<li>A functional REST API for loan default predictions.</li>\n",
    "<li>A final presentation summarizing the project findings.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1>Data Collection and Preparation:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "SIZE_OF_SAMPLES = 1000000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejected Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 100,000 rows of the dataset\n",
    "file_path_accepted = r\"./rejected_2007_to_2018Q4.csv\"\n",
    "#df = pd.read_csv(file_path_accepted, nrows=SIZE_OF_SAMPLES)\n",
    "df = pd.read_csv(file_path_accepted)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical columns with the median\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Fill categorical columns with the most frequent value (mode)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "\n",
    "print(f\"Before Drop Columns with Too Many Missing Values (e.g., > 50%): {df.shape}\")\n",
    "threshold = len(df.columns) // 2\n",
    "df = df.dropna(thresh=threshold)\n",
    "# 3. Drop Columns with Too Many Missing Values (e.g., > 50%)\n",
    "missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "print(missing_percentage)\n",
    "columns_to_drop = missing_percentage[missing_percentage > 50].index\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "#df = df.drop(columns=['id'])\n",
    "print(f\"After Drop Columns with Too Many Missing Values (e.g., > 50%): {df.shape}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "#Encode the loan title\n",
    "label_encoder = LabelEncoder()\n",
    "df['Loan_Title_Encoded'] = label_encoder.fit_transform(df['Loan Title'])\n",
    "df.head()\n",
    "\n",
    "df['zip_code'] = df['Zip Code'].str.replace('xx', '').astype(float)\n",
    "\n",
    "df['emp_length'] = df['Employment Length'].replace({'< 1 year': '0','10+ years': '11'}).str.extract('(\\d+)').astype(float)\n",
    "df['Debt-To-Income Ratio'] = df['Debt-To-Income Ratio'].str.rstrip('%').astype(float) / 100\n",
    "df['Debt-To-Income Ratio'] = df['Debt-To-Income Ratio'].apply(lambda x: x + 1 if x == 0 else x)\n",
    "df['Annual_income'] =  df['Amount Requested'] / df['Debt-To-Income Ratio']\n",
    "df['policy_code'] = df['Policy Code']\n",
    "\n",
    "\n",
    "df['Status'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non used columns\n",
    "df['Amount_Requested'] = df['Amount Requested']\n",
    "df['income_debt_ratio'] = df['Debt-To-Income Ratio']\n",
    "columnsToDrop = ['Amount Requested', 'Application Date', 'Loan Title', 'Debt-To-Income Ratio', 'Zip Code', 'State', 'Employment Length', 'Policy Code']\n",
    "df = df.drop(columns=columnsToDrop)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save the Cleaned Dataset\n",
    "output_path_rejected = f\"./rejected_cleaned_{SIZE_OF_SAMPLES}_rows.csv\"\n",
    "df.to_csv(output_path_rejected, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path_rejected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accepted Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 100,000 rows of the dataset\n",
    "file_path_accepted = r\"./accepted_2007_to_2018Q4.csv\"\n",
    "#df = pd.read_csv(file_path_accepted, nrows=SIZE_OF_SAMPLES)\n",
    "df = pd.read_csv(file_path_accepted)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before Drop Columns with Too Many Missing Values (e.g., > 50%): {df.shape}\")\n",
    "threshold = len(df.columns) // 2\n",
    "df = df.dropna(thresh=threshold)\n",
    "# 3. Drop Columns with Too Many Missing Values (e.g., > 50%)\n",
    "missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "print(missing_percentage)\n",
    "columns_to_drop = missing_percentage[missing_percentage > 10].index\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "df = df.drop(columns=['id'])\n",
    "print(f\"After Drop Columns with Too Many Missing Values (e.g., > 50%): {df.shape}\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle Missing Values\n",
    "df['zip_code'] = df['zip_code'].str.replace('xx', '').astype(float)\n",
    "df['term'] = df['term'].replace('< 1 year', '0').str.extract('(\\d+)').astype(int)\n",
    "grade_mapping = {\n",
    "    'A': 4,\n",
    "    'B': 3,\n",
    "    'C': 2,\n",
    "    'D': 1,\n",
    "    'F': 0\n",
    "}\n",
    "\n",
    "# Convert grades to numerical values\n",
    "df['grade'] = df['grade'].map(grade_mapping)\n",
    "\n",
    "df['sub_grade'] = df['sub_grade'].str[1:]\n",
    "\n",
    "df['emp_length'] = df['emp_length'].replace({'< 1 year': '0','10+ years': '11'}).str.extract('(\\d+)').astype(float)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['Loan_Title_Encoded'] = label_encoder.fit_transform(df['title'])\n",
    "\n",
    "# Fill numerical columns with the median\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Fill categorical columns with the most frequent value (mode)\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "\n",
    "df['Status'] = 'Approved'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove Duplicates \n",
    "print(f\"Before remove duplicates: {df.shape}\")\n",
    "df = df.drop_duplicates()\n",
    "print(f\"After remove duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Create the same strutucture to validate the data\n",
    "df_cleaned = pd.DataFrame(columns=['Risk_Score','Loan_Title_Encoded','zip_code','emp_length','Annual_income','policy_code','Status','Amount_Requested','income_debt_ratio'])\n",
    "\n",
    "df_cleaned['Amount_Requested'] = df['loan_amnt']\n",
    "df_cleaned['emp_length'] = df['emp_length']\n",
    "df_cleaned['Annual_income'] = df['annual_inc']\n",
    "df_cleaned['Annual_income'].fillna(0, inplace=True)\n",
    "df_cleaned['zip_code'] = df['zip_code']\n",
    "df_cleaned['policy_code'] = df['policy_code']\n",
    "df_cleaned['Risk_Score'] = (df['fico_range_low'] + df['fico_range_high'])/2\n",
    "df_cleaned['income_debt_ratio']  = round(df_cleaned['Amount_Requested'] / df_cleaned['Annual_income'],3)\n",
    "df_cleaned['income_debt_ratio'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df_cleaned['Loan_Title_Encoded'] = df['Loan_Title_Encoded']\n",
    "\n",
    "# Example: Preprocessing the target variable\n",
    "def preprocess_target_status(status):\n",
    "    return 0 if status in ['Charged Off', 'Defaulted'] else 1\n",
    "\n",
    "# Create the binary target variable\n",
    "df_cleaned['Status'] = df['loan_status'].apply(preprocess_target_status)\n",
    "df_cleaned = df_cleaned[df_cleaned['Status'] != 0]\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "#df.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save the Cleaned Dataset\n",
    "output_path_accepted = f\"./accepted_cleaned_{SIZE_OF_SAMPLES}_rows.csv\"\n",
    "df_cleaned.to_csv(output_path_accepted, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path_accepted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Variable Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configure visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the first 50,000 rows of each dataset\n",
    "accepted_data = pd.read_csv(output_path_accepted, nrows=SIZE_OF_SAMPLES)\n",
    "rejected_data = pd.read_csv(output_path_rejected, nrows=SIZE_OF_SAMPLES)\n",
    "\n",
    "# Combine the two datasets\n",
    "df = pd.concat([accepted_data, rejected_data], ignore_index=True)\n",
    "\n",
    "# Inspect the first few rows to verify the data\n",
    "df.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Dataset Overview\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"Sample Rows:\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Missing Values Analysis\n",
    "missing = df.isnull().sum()\n",
    "missing_percentage = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing, 'Percentage': missing_percentage})\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nColumns with Missing Values:\")\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_df.shape[0] > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
    "    plt.title(\"Missing Values Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Descriptive Statistics\n",
    "print(\"\\nStatistical Summary (Numerical Columns):\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 4. Univariate Analysis\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Filter numerical columns to exclude empty or constant columns\n",
    "valid_numerical_cols = [col for col in numerical_cols if df[col].nunique() > 1 and not df[col].isnull().all()]\n",
    "\n",
    "# Numerical data distributions\n",
    "num_columns = 2\n",
    "num_features =len(valid_numerical_cols)\n",
    "num_rows = math.ceil(num_features/num_columns)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_columns, figsize=(16, 5 * num_rows))\n",
    "axes = axes.flatten()  # Flatten axes to iterate easily\n",
    "\n",
    "for i, col in enumerate(valid_numerical_cols):\n",
    "    if df[col].dropna().nunique() > 1:  # Ensure column has sufficient variation\n",
    "        df[col].hist(ax=axes[i], bins=8, edgecolor=\"black\", color='skyblue')\n",
    "        axes[i].set_title(f\"Distribution of {col}\")\n",
    "plt.show()\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Categorical Columns')\n",
    "# Categorical data distributions (Top 5)\n",
    "for col in categorical_cols[:5]:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10], palette='muted')\n",
    "    plt.title(f\"Distribution of Top Categories for {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Bivariate Analysis\n",
    "# Correlation Heatmap\n",
    "corr = df[valid_numerical_cols].corr()\n",
    "\n",
    "# Filter meaningful correlations\n",
    "strong_corr = corr[(corr > 0.5) & (corr < 1)].stack().reset_index()\n",
    "strong_corr.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "#print(\"\\nStrong Correlations:\")\n",
    "#print(strong_corr.sort_values(by='Correlation', ascending=False)[:10])\n",
    "\n",
    "# Numerical data distributions\n",
    "num_columns = 2\n",
    "num_features =20\n",
    "num_rows = math.ceil(num_features/num_columns)\n",
    "\n",
    "plt.figure(figsize=(15, 5*num_rows))\n",
    "\n",
    "# Plot scatter plots for each pair of features\n",
    "i = 1\n",
    "for _, row in strong_corr.sort_values(by='Correlation', ascending=False)[:20].iterrows():\n",
    "    \n",
    "    feature1, feature2 = row['Feature 1'], row['Feature 2']\n",
    "    if feature1 in df.columns and feature2 in df.columns:\n",
    "        plt.subplot(num_rows, num_columns, i)\n",
    "        sns.scatterplot(x=df[feature1], y=df[feature2])\n",
    "        plt.title(f\"Correlation between {feature1} and {feature2}\")\n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "        \n",
    "    else:\n",
    "        print(f\"One or both features ({feature1}, {feature2}) not found in the dataset.\")\n",
    "    i = i + 1\n",
    "plt.show()\n",
    "# Plotting correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outlier Detection\n",
    "import math\n",
    "\n",
    "num_columns = 3\n",
    "num_features =len(valid_numerical_cols)\n",
    "num_rows = math.ceil(num_features/num_columns)\n",
    "\n",
    "# Boxplots for Outliers (Only for Valid Numerical Columns)\n",
    "\n",
    "plt.figure(figsize=(15, 5*num_rows))\n",
    "for i, col in enumerate(valid_numerical_cols, start=1):\n",
    "    if df[col].dropna().nunique() > 1:  # Ensure column has sufficient variation\n",
    "        plt.subplot(num_rows, num_columns, i)\n",
    "        sns.boxplot(y=col, data=df, color='lightblue')\n",
    "        plt.title(f\"Outlier Detection: {col}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Data Cleaning Example (Missing Values)\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Fill missing values (example: median imputation for numerical columns)\n",
    "for col in valid_numerical_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "print(f\"\\nDataset Shape After Cleaning: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Feature Engineering Example\n",
    "# Final Dataset Preview\n",
    "print(\"\\nFinal Cleaned Dataset Preview:\")\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEDA Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop columns irrelevant to the prediction\n",
    "X = df_cleaned.drop(columns=['Status'])  # Drop target and ID\n",
    "y = df_cleaned['Status']  # Set target column\n",
    "\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Display the cross-validation results\n",
    "cv_results = {\n",
    "    \"Fold\": [f\"Fold {i+1}\" for i in range(len(cv_scores))],\n",
    "    \"Accuracy\": cv_scores\n",
    "}\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "cv_results_df.loc[\"Mean\"] = [\"Mean\", cv_scores.mean()]\n",
    "cv_results_df.loc[\"Std\"] = [\"Std Dev\", cv_scores.std()]\n",
    "cv_results_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Train the Decision Tree Model\n",
    "dt_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, feature_names=X_train.columns, class_names=[\"Not Default\", \"Default\"], filled=True, fontsize=10)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Metrics\n",
    "data = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"XGBoost\", \"Decision Tree\"],\n",
    "    \"Accuracy\": [1.000, 0.9910, 0.9945, 0.9795],\n",
    "    \"Precision (Class 1)\": [1.00, 1.00, 0.99, 0.94],\n",
    "    \"Recall (Class 1)\": [1.00, 0.94, 0.97, 0.93],\n",
    "    \"F1-Score (Class 1)\": [1.00, 0.97, 0.98, 0.93],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(data)\n",
    "\n",
    "# Visualize the Metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric in [\"Accuracy\", \"Precision (Class 1)\", \"Recall (Class 1)\", \"F1-Score (Class 1)\"]:\n",
    "    plt.plot(comparison_df[\"Model\"], comparison_df[metric], marker=\"o\", label=metric)\n",
    "\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used four models to make predictions about loan defaults: Logistic Regression, Random Forest, XGBoost, and Decision Tree. These models were evaluated based on their accuracy, precision, recall, F1-score, and confusion matrices.\n",
    "\n",
    "Logistic Regression achieved perfect classification scores. While this might seem like the best model, the perfect scores actually indicate overfitting. This model is unlikely to generalize well to unseen data and struggles to capture the non-linear relationships within the dataset.\n",
    "\n",
    "Random Forest performed quite well, achieving an accuracy of 99.1%, precision of 1.0, and recall of 0.94. This model is better equipped to identify non-linear relationships, is robust to imbalanced data, and provides highly interpretable feature importances.\n",
    "\n",
    "XGBoost outperformed Random Forest with an accuracy of 99.45% and a higher recall score of 0.97, demonstrating superior ability to reduce false negatives. This model is extremely scalable and excels at capturing complex patterns, making it the most suitable model for this task.\n",
    "\n",
    "Decision Tree had the lowest accuracy at 97.95% and the lowest recall at 0.93, suggesting it is more prone to overfitting. While simple and interpretable, the model’s simplicity comes at the cost of robustness and generalization.\n",
    "\n",
    "In conclusion, XGBoost is the best model due to its superior recall and adaptability, followed by Random Forest for its balance of simplicity and performance. Decision Tree in the last option because its robust for the dataset. Logistic Regression can serve as a baseline but should not be used for predictions as it tends to overfit to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameter Tuning using  RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test set accuracy with the best model: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test set accuracy with the best model: {test_score:.4f}\")\n",
    "best_model.save('FinalModel.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used `RandomizedSearchCV` for hyperparameter tuning because it allows us to efficiently explore the hyperparameter space of complex models like RandomForestClassifier and XGBClassifier. By randomly sampling a subset of possible configurations, it reduces the computational cost and risk of overfitting, while still identifying promising hyperparameter combinations. This approach enables us to achieve better model performance with fewer computational resources, making it ideal for optimizing the models in practical applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
